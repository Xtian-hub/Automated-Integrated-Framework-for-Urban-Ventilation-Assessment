{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4016f02",
   "metadata": {},
   "source": [
    "Automated Computational Framework for Multi-Element Urban Ventilation Assessment: Semantic Segmentation and CFD-Ready Geometry Reconstruction\n",
    "<br> 作者：耿晓天"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fbc4dc",
   "metadata": {},
   "source": [
    "## 三维纹理网格特征提取计算\n",
    "输入路径下需同时包含obj, mtl, jpg，支持多纹理图集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9c2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from plyfile import PlyData, PlyElement\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from meshfea import MeshFeature_Extraction as MFE\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "obj_file_path = r\"I:\\CFD\\highbuilding\\mesh\"\n",
    "output_dir = r\"J:\\paper_cfd_code\\test\"\n",
    "\n",
    "def process_obj_feature(obj_file_path):\n",
    "    print(f\"Processing: {obj_file_path}\")\n",
    "    try:\n",
    "        mesh = o3d.io.read_triangle_mesh(obj_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {obj_file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    if mesh.is_empty():\n",
    "        print(f\"Mesh is empty or failed to load: {obj_file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 构造 MeshFeature_Extraction 对象\n",
    "        mesh_features = MFE(obj_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing features for {obj_file_path}: {e}\")\n",
    "        return\n",
    "    # ---------------特征提取---------------------\n",
    "    # 提取顶点与三角形索引\n",
    "    vertices = np.asarray(mesh.vertices)\n",
    "    triangles = np.asarray(mesh.triangles)\n",
    "    # 计算每个三角形的中心点（points）及法向量（normal）\n",
    "    triangle_centroids = np.mean(vertices[triangles], axis=1)  # shape: (num_faces, 3)\n",
    "    mesh.compute_triangle_normals()\n",
    "    cen_normal = np.asarray(mesh.triangle_normals)   \n",
    "    # 初始化各子特征计算对象\n",
    "    geometric_features = MFE.Geometric_Features(mesh_features)\n",
    "    elevation_features = MFE.Elevation_Features(mesh_features)\n",
    "    color_features = MFE.Color_Features(mesh_features)\n",
    "\n",
    "    # --- 计算面（即三角形）级别的特征 ---\n",
    "\n",
    "    try:\n",
    "        areas = geometric_features.mesh_area()\n",
    "        _area_mean, _area_variance = geometric_features.calculate_triangle_area_variance_and_mean()\n",
    "        _flatness = geometric_features.mesh_flatness()\n",
    "        _density = geometric_features.mesh_density()\n",
    "        _verticality = geometric_features.mesh_verticality()\n",
    "        _masball_radius = geometric_features.triangleCenter_masb_raduis()\n",
    "        _masball_radius_mean, _masball_radius_variance = geometric_features.masb_radius_mean_var()\n",
    "        _height = elevation_features.pointHeight2Ground()\n",
    "        _mesh_hvi = color_features.calculate_triangle_hvi()\n",
    "        hsv_hist = color_features.calculate_triangle_hsv_histograms()\n",
    "        triangle_hsv_stats = color_features.calculate_triangle_hsv_mean_std()\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing features for {obj_file_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    num_faces = len(triangles)\n",
    "    if len(areas) != num_faces:\n",
    "        print(f\"Mismatch: computed {len(areas)} faces != {num_faces} triangles\")\n",
    "        return\n",
    "\n",
    "    points = np.array([np.array(p) for p in triangle_centroids]) # shape: (num_faces, 3) # 3 0-2\n",
    "    normal = np.array([np.array(n) for n in cen_normal]) # shape: (num_faces, 3) # 3 3-5\n",
    "    area = np.expand_dims(areas, axis=1)  # shape: (num_faces, 1) # 1 6\n",
    "    area_mean = np.expand_dims(_area_mean, axis=1)  # shape: (num_faces, 1) # 1 7\n",
    "    area_variance = np.expand_dims(_area_variance, axis=1)  # shape: (num_faces, 1) # 1 8\n",
    "    flatness = np.expand_dims(_flatness, axis=1)  # shape: (num_faces, 1) # 1 9 \n",
    "    density = np.expand_dims(_density, axis=1)  # shape: (num_faces, 1) # 1 10\n",
    "    verticality = np.expand_dims(_verticality, axis=1)  # shape: (num_faces, 1) # 1 11\n",
    "    masball_radius = np.expand_dims(_masball_radius, axis=1)  # shape: (num_faces, 1) # 1 12\n",
    "    masball_radius_mean = np.expand_dims(_masball_radius_mean, axis=1)  # shape: (num_faces, 1) # 1 13\n",
    "    masball_radius_variance = np.expand_dims(_masball_radius_variance, axis=1)  # shape: (num_faces, 1) # 1 14\n",
    "    height = np.expand_dims(_height, axis=1)  # shape: (num_faces, 1) # 1 15\n",
    "    mesh_hvi = np.expand_dims(_mesh_hvi, axis=1)  # shape: (num_faces, 1) # 1 16 \n",
    "    \n",
    "    # Expand all hsv_hist features (assuming they are 1D arrays)\n",
    "    hsv_hist_0 = np.expand_dims(np.asarray(hsv_hist)[:,0], axis=1)  # shape: (num_faces, 1) # 1 17\n",
    "    hsv_hist_1 = np.expand_dims(np.asarray(hsv_hist)[:,1], axis=1)  # shape: (num_faces, 1) # 1 18\n",
    "    hsv_hist_2 = np.expand_dims(np.asarray(hsv_hist)[:,2], axis=1)  # shape: (num_faces, 1) # 1 19\n",
    "    hsv_hist_3 = np.expand_dims(np.asarray(hsv_hist)[:,3], axis=1)  # shape: (num_faces, 1) # 1 20\n",
    "    hsv_hist_4 = np.expand_dims(np.asarray(hsv_hist)[:,4], axis=1)  # shape: (num_faces, 1) # 1 21\n",
    "    hsv_hist_5 = np.expand_dims(np.asarray(hsv_hist)[:,5], axis=1)  # shape: (num_faces, 1) # 1 22\n",
    "    hsv_hist_6 = np.expand_dims(np.asarray(hsv_hist)[:,6], axis=1)  # shape: (num_faces, 1) # 1 23\n",
    "    hsv_hist_7 = np.expand_dims(np.asarray(hsv_hist)[:,7], axis=1)  # shape: (num_faces, 1) # 1 24\n",
    "    hsv_hist_8 = np.expand_dims(np.asarray(hsv_hist)[:,8], axis=1)  # shape: (num_faces, 1) # 1 25\n",
    "    hsv_hist_9 = np.expand_dims(np.asarray(hsv_hist)[:,9], axis=1)  # shape: (num_faces, 1) # 1 26\n",
    "    hsv_hist_10 = np.expand_dims(np.asarray(hsv_hist)[:,10], axis=1)  # shape: (num_faces, 1) # 1 27\n",
    "    hsv_hist_11 = np.expand_dims(np.asarray(hsv_hist)[:,11], axis=1)  # shape: (num_faces, 1) # 1 28\n",
    "    hsv_hist_12 = np.expand_dims(np.asarray(hsv_hist)[:,12], axis=1)  # shape: (num_faces, 1) # 1 29\n",
    "    hsv_hist_13 = np.expand_dims(np.asarray(hsv_hist)[:,13], axis=1)  # shape: (num_faces, 1) # 1 30\n",
    "    hsv_hist_14 = np.expand_dims(np.asarray(hsv_hist)[:,14], axis=1)  # shape: (num_faces, 1) # 1 31\n",
    "    hsv_hist_15 = np.expand_dims(np.asarray(hsv_hist)[:,15], axis=1)  # shape: (num_faces, 1) # 1 32\n",
    "    hsv_hist_16 = np.expand_dims(np.asarray(hsv_hist)[:,16], axis=1)  # shape: (num_faces, 1) # 1 33\n",
    "    hsv_hist_17 = np.expand_dims(np.asarray(hsv_hist)[:,17], axis=1)  # shape: (num_faces, 1) # 1 34\n",
    "    hsv_hist_18 = np.expand_dims(np.asarray(hsv_hist)[:,18], axis=1)  # shape: (num_faces, 1) # 1 35\n",
    "    hsv_hist_19 = np.expand_dims(np.asarray(hsv_hist)[:,19], axis=1)  # shape: (num_faces, 1) # 1 36\n",
    "    hsv_hist_20 = np.expand_dims(np.asarray(hsv_hist)[:,20], axis=1)  # shape: (num_faces, 1) # 1 37\n",
    "    hsv_hist_21 = np.expand_dims(np.asarray(hsv_hist)[:,21], axis=1)  # shape: (num_faces, 1) # 1 38\n",
    "    hsv_hist_22 = np.expand_dims(np.asarray(hsv_hist)[:,22], axis=1)  # shape: (num_faces, 1) # 1 39\n",
    "    hsv_hist_23 = np.expand_dims(np.asarray(hsv_hist)[:,23], axis=1)  # shape: (num_faces, 1) # 1 40\n",
    "    hsv_hist_24 = np.expand_dims(np.asarray(hsv_hist)[:,24], axis=1)  # shape: (num_faces, 1) # 1 41\n",
    "\n",
    "\n",
    "    # Triangle HSV stats\n",
    "    triangle_hsv_stats_0 = np.expand_dims(np.asarray(triangle_hsv_stats)[:,0], axis=1)  # shape: (num_faces, 1) # 1 42\n",
    "    triangle_hsv_stats_1 = np.expand_dims(np.asarray(triangle_hsv_stats)[:,1], axis=1)  # shape: (num_faces, 1) # 1 43 \n",
    "    triangle_hsv_stats_2 = np.expand_dims(np.asarray(triangle_hsv_stats)[:,2], axis=1)  # shape: (num_faces, 1) # 1 44\n",
    "    triangle_hsv_stats_3 = np.expand_dims(np.asarray(triangle_hsv_stats)[:,3], axis=1)  # shape: (num_faces, 1) # 1 45\n",
    "    triangle_hsv_stats_4 = np.expand_dims(np.asarray(triangle_hsv_stats)[:,4], axis=1)  # shape: (num_faces, 1) # 1 46\n",
    "    triangle_hsv_stats_5 = np.expand_dims(np.asarray(triangle_hsv_stats)[:,5], axis=1)  # shape: (num_faces, 1) # 1 47\n",
    "    # 返回拼接后的特征\n",
    "    mesh_features = np.concatenate((\n",
    "        points, normal, area, area_mean, area_variance, flatness, density, verticality,\n",
    "        masball_radius, masball_radius_mean, masball_radius_variance, height, mesh_hvi,\n",
    "        hsv_hist_0, hsv_hist_1, hsv_hist_2, hsv_hist_3, hsv_hist_4, hsv_hist_5, hsv_hist_6,\n",
    "        hsv_hist_7, hsv_hist_8, hsv_hist_9, hsv_hist_10, hsv_hist_11, hsv_hist_12, hsv_hist_13,\n",
    "        hsv_hist_14, hsv_hist_15, hsv_hist_16, hsv_hist_17, hsv_hist_18, hsv_hist_19, hsv_hist_20,\n",
    "        hsv_hist_21, hsv_hist_22, hsv_hist_23, hsv_hist_24, triangle_hsv_stats_0, triangle_hsv_stats_1,\n",
    "        triangle_hsv_stats_2, triangle_hsv_stats_3, triangle_hsv_stats_4, triangle_hsv_stats_5\n",
    "    ), axis=1)\n",
    "    return mesh_features\n",
    "\n",
    "skipped_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(obj_file_path):\n",
    "    obj_files = [f for f in files if f.endswith(\".obj\")]\n",
    "    for obj_file in tqdm(obj_files, desc=\"Processing OBJ files\"):\n",
    "        identifier = os.path.splitext(os.path.basename(obj_file))[0].rsplit('_', 1)[0]\n",
    "        outh5_identifier = os.path.splitext(os.path.basename(obj_file))[0]\n",
    "        out_h5file_path = os.path.join(output_dir, outh5_identifier + \".h5\")\n",
    "\n",
    "        if os.path.exists(out_h5file_path):\n",
    "            print(f\"Output H5 file for {identifier} already exists. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            mesh_fea = process_obj_feature(os.path.join(obj_file_path, outh5_identifier + '.obj'))\n",
    "            if mesh_fea is None:\n",
    "                raise ValueError(\"Feature extraction returned None\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {identifier}: {e}. Skipping file.\")\n",
    "            skipped_files.append(identifier)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with h5py.File(out_h5file_path, 'w') as hf:\n",
    "                hf.create_dataset('data', data=mesh_fea)\n",
    "            print(f\"Saved H5 file: {out_h5file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error writing H5 for {obj_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "with open(\"skipped_files_train.txt\", \"w\") as f:\n",
    "    for fname in skipped_files:\n",
    "        f.write(fname + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d24294",
   "metadata": {},
   "source": [
    "## 文件切分，避免文件过大模型无法读入数据\n",
    "输入路径下需要留h5格式文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b22924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import h5py\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "h5_root     = r\"J:\\paper_cfd_code\\test\"\n",
    "output_base = r\"J:\\paper_cfd_code\\test\"\n",
    "MIN_ROWS    = 81920     # 保留之前的最小行数阈值\n",
    "MAX_BYTES   = 1 * 1024 * 1024  # 1 MB\n",
    "\n",
    "def extract_scene_name(filename):\n",
    "    base = os.path.splitext(filename)[0]\n",
    "    tokens = base.split(\"_\")\n",
    "    if len(tokens) >= 3:\n",
    "        return f\"{tokens[1]}_{tokens[2]}_{tokens[-1]}\"\n",
    "    return base\n",
    "\n",
    "def split_and_save_h5(h5_path, output_root):\n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        data_array = f[\"data\"][:]\n",
    "    # 跳过过小的数据\n",
    "    if data_array.shape[0] < MIN_ROWS:\n",
    "        print(f\"  [SKIP] {os.path.basename(h5_path)} has only {data_array.shape[0]} rows\")\n",
    "        return\n",
    "\n",
    "    # 拆分份数由 color.npy 大小决定\n",
    "    mesh_hvi       = data_array[:, 6:].astype(np.float32)\n",
    "    total_bytes    = mesh_hvi.nbytes\n",
    "    num_parts      = math.ceil(total_bytes / MAX_BYTES) if total_bytes > MAX_BYTES else 1\n",
    "\n",
    "    scene_name = extract_scene_name(os.path.basename(h5_path))\n",
    "    parts_data = np.array_split(data_array, num_parts)\n",
    "    print(f\"  Splitting into {num_parts} parts (color bytes={total_bytes})\")\n",
    "\n",
    "    for idx, data_part in enumerate(parts_data, start=1):\n",
    "        subfolder_name = f\"{scene_name}_{idx:03d}\"\n",
    "        subfolder_path = os.path.join(output_root, subfolder_name)\n",
    "        os.makedirs(subfolder_path, exist_ok=True)\n",
    "\n",
    "        points   = data_part[:, 0:3].astype(np.float32)\n",
    "        normals  = data_part[:, 3:6].astype(np.float32)\n",
    "        mesh_hvi = data_part[:, 6:].astype(np.float32)\n",
    "\n",
    "        np.save(os.path.join(subfolder_path, \"coord.npy\"), points)\n",
    "        np.save(os.path.join(subfolder_path, \"normal.npy\"), normals)\n",
    "        np.save(os.path.join(subfolder_path, \"color.npy\"), mesh_hvi)\n",
    "\n",
    "def rename_folders(root_dir):\n",
    "    all_folders = sorted(os.listdir(root_dir))\n",
    "    groups = defaultdict(list)\n",
    "    for folder in all_folders:\n",
    "        full_path = os.path.join(root_dir, folder)\n",
    "        if os.path.isdir(full_path):\n",
    "            parts = folder.split('_')\n",
    "            group_key = '_'.join(parts[:-1]) if len(parts) >= 2 else folder\n",
    "            groups[group_key].append(folder)\n",
    "\n",
    "    new_names = {}\n",
    "    for group_id, key in enumerate(sorted(groups.keys())):\n",
    "        group_folders = groups[key]\n",
    "        try:\n",
    "            group_folders.sort(key=lambda x: int(x.split('_')[-1]))\n",
    "        except ValueError:\n",
    "            group_folders.sort()\n",
    "        for intra_index, folder in enumerate(group_folders):\n",
    "            new_names[folder] = f\"scene{group_id:04d}_{intra_index:02d}\"\n",
    "\n",
    "    for old_name, new_name in new_names.items():\n",
    "        old_path = os.path.join(root_dir, old_name)\n",
    "        new_path = os.path.join(root_dir, new_name)\n",
    "        print(f\"Renaming: {old_name} -> {new_name}\")\n",
    "        os.rename(old_path, new_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    print(\"=== Processing H5 files with dynamic split for color.npy <= 1MB ===\")\n",
    "    for file in sorted(os.listdir(h5_root)):\n",
    "        if not file.lower().endswith(\".h5\"):\n",
    "            continue\n",
    "        print(f\"Processing {file}\")\n",
    "        split_and_save_h5(os.path.join(h5_root, file), output_base)\n",
    "\n",
    "    print(\"\\nSplitting complete. Start renaming…\")\n",
    "    rename_folders(output_base)\n",
    "    print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5904ad6f",
   "metadata": {},
   "source": [
    "## 植被颜色抖动\n",
    "对植被颜色进行随机抖动，以增加数据的多样性和鲁棒性。<br>SUM数据集中植被颜色全部为绿色，这一现象会导致模型泛化性不足。<br>在SUM数据集中，植被的标签为2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82daa926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import torchvision.transforms.functional as F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import trimesh\n",
    "from PIL import Image, ImageOps\n",
    "# =============================================================================\n",
    "# 1. 根据OBJ文件名寻找对应的TXT文件，并加载点云及分类数据\n",
    "# =============================================================================\n",
    "\n",
    "def label_jitter(obj_file, txt_file_path, output_path, jitter_label):\n",
    "    identifier = os.path.splitext(os.path.basename(obj_file))[0]\n",
    "    output_objpath = os.path.join(output_path, identifier + \"_jittered.obj\")\n",
    "    output_texturepath = os.path.join(identifier + \"_jittered\")  # 如有纹理，可用 JPG 或 PNG\n",
    "    if os.path.exists(output_objpath):\n",
    "            print(f\"导出文件 {output_objpath} 已存在，跳过。\")\n",
    "            return\n",
    "    def extract_patches_with_bbox(combined_img):\n",
    "        # 构造二值掩膜（非黑区域）\n",
    "        mask = np.any(combined_img != 0, axis=2).astype(np.uint8) * 255\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        patch_infos = []\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            patch = combined_img[y:y+h, x:x+w]\n",
    "            patch_infos.append((patch, x, y, w, h))\n",
    "        return patch_infos\n",
    "\n",
    "    class AsymmetricHueJitter:\n",
    "        def __init__(self, hue_range):\n",
    "            self.hue_min, self.hue_max = hue_range\n",
    "        def __call__(self, img: Image.Image) -> Image.Image:\n",
    "            hue_factor = random.uniform(self.hue_min, self.hue_max)\n",
    "            return F.adjust_hue(img, hue_factor)\n",
    "            \n",
    "    matching_txt_files = []\n",
    "    for root, dirs, files in os.walk(txt_file_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') and identifier in file:\n",
    "                matching_txt_files.append(os.path.join(root, file))\n",
    "    new_mesh = None\n",
    "\n",
    "    # =============================================================================\n",
    "    # 2. 针对每个匹配的TXT文件，提取class==2的三角形\n",
    "    # =============================================================================\n",
    "    if not matching_txt_files:\n",
    "        print(\"没有匹配的 TXT 文件，直接返回。\")\n",
    "        return\n",
    "\n",
    "    for txt_file in matching_txt_files:\n",
    "        print(f\"标签匹配中\")\n",
    "        with open(txt_file, \"r\") as f:\n",
    "            lines = [line.strip() for line in f.readlines() if not line.startswith(\"//\")]\n",
    "        data = np.loadtxt(lines)\n",
    "        df = pd.DataFrame(data, columns=['X_COG', 'Y_COG', 'Z_COG', 'classification'])\n",
    "        point_cloud = df[['X_COG', 'Y_COG', 'Z_COG']].values\n",
    "        labels = df['classification'].values\n",
    "\n",
    "        # 读取OBJ网格\n",
    "        read_mesh = o3d.io.read_triangle_mesh(obj_file)\n",
    "        vertices = np.asarray(read_mesh.vertices)\n",
    "        triangles = np.asarray(read_mesh.triangles)\n",
    "\n",
    "        # 计算每个三角形的质心\n",
    "        mesh_fea_center = np.mean(vertices[triangles], axis=1)\n",
    "\n",
    "        # 利用最近邻为每个三角形质心分配标签\n",
    "        nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(point_cloud)\n",
    "        distances, indices = nbrs.kneighbors(mesh_fea_center)\n",
    "        triangle_labels = labels[indices.flatten()]\n",
    "\n",
    "        # 提取标签为2的三角形索引\n",
    "        veg_triangle = [i for i, label in enumerate(triangle_labels) if label == jitter_label]\n",
    "        # print(f\"共找到 {len(veg_triangle)} 个标签为 2 的三角形。\")\n",
    "        if len(veg_triangle) == 0:\n",
    "            print(\"无标签为2的三角形，跳过可视化。\")\n",
    "            return\n",
    "\n",
    "        # 从原网格中提取这些三角形及对应顶点\n",
    "        veg_triangles = triangles[veg_triangle]\n",
    "        veg_vertices_idx = np.unique(veg_triangles.flatten())\n",
    "        veg_vertices = vertices[veg_vertices_idx]\n",
    "        map_old2new = {old: new for new, old in enumerate(veg_vertices_idx)}\n",
    "        veg_triangles_new = np.array([[map_old2new[idx] for idx in tri] for tri in veg_triangles], dtype=np.int32)\n",
    "\n",
    "        # 构造新的网格对象（只包含class==2的三角形）\n",
    "        mesh_veg = o3d.geometry.TriangleMesh()\n",
    "        mesh_veg.vertices = o3d.utility.Vector3dVector(veg_vertices)\n",
    "        mesh_veg.triangles = o3d.utility.Vector3iVector(veg_triangles_new)\n",
    "\n",
    "        # 如果原网格有UV，则提取对应的纹理坐标\n",
    "        if read_mesh.triangle_uvs is not None and len(read_mesh.triangle_uvs) > 0:\n",
    "            orig_uvs = np.asarray(read_mesh.triangle_uvs)\n",
    "            veg_uvs = []\n",
    "            for t in veg_triangle:\n",
    "                base_idx = t * 3\n",
    "                veg_uvs.extend([orig_uvs[base_idx + 0], orig_uvs[base_idx + 1], orig_uvs[base_idx + 2]])\n",
    "            veg_uvs = np.array(veg_uvs, dtype=np.float64)\n",
    "            mesh_veg.triangle_uvs = o3d.utility.Vector2dVector(veg_uvs)\n",
    "\n",
    "        # 复制材质索引（如果有）\n",
    "        if read_mesh.triangle_material_ids is not None and len(read_mesh.triangle_material_ids) > 0:\n",
    "            orig_mat_ids = np.asarray(read_mesh.triangle_material_ids)\n",
    "            veg_mat_ids = orig_mat_ids[veg_triangle]\n",
    "            mesh_veg.triangle_material_ids = o3d.utility.IntVector(veg_mat_ids.tolist())\n",
    "\n",
    "        # 保留原始纹理贴图（这里使用纹理1）\n",
    "        if read_mesh.textures is not None and len(read_mesh.textures) > 0:\n",
    "            meshVeg_textures = []\n",
    "            meshVeg_textures.append(read_mesh.textures[1])\n",
    "            meshVeg_textures.append(read_mesh.textures[1])\n",
    "            for i in range(2, len(read_mesh.textures) - 1):\n",
    "                meshVeg_textures.append(read_mesh.textures[i])\n",
    "            mesh_veg.textures = meshVeg_textures\n",
    "\n",
    "        # =============================================================================\n",
    "        # 3. 针对每个三角形提取纹理区域，并生成单独图片（仅保留三角形内纹理，其余为黑色）\n",
    "        # =============================================================================\n",
    "        # 构造纹理列表（假设read_mesh.textures中存放的是PIL.Image）\n",
    "        mesh_textures = []\n",
    "        mesh_textures.append(read_mesh.textures[1])\n",
    "        mesh_textures.append(read_mesh.textures[1])\n",
    "        for i in range(2, len(read_mesh.textures) - 1):\n",
    "            mesh_textures.append(read_mesh.textures[i])\n",
    "\n",
    "        # 先将每张纹理转换为HSV格式（便于后续处理）\n",
    "        hsv_textures = []\n",
    "        for texture in mesh_textures:\n",
    "            texture_np = np.asarray(texture)\n",
    "            texture_hsv = cv2.cvtColor(texture_np, cv2.COLOR_RGB2HSV)\n",
    "            hsv_textures.append(texture_hsv)\n",
    "\n",
    "        triangle_texture_images = []\n",
    "        uvs_all = np.asarray(mesh_veg.triangle_uvs).reshape(-1, 3, 2)\n",
    "        mat_ids_all = np.asarray(mesh_veg.triangle_material_ids)\n",
    "        print('纹理拼接中')\n",
    "        for i, (uv_coords, material_id) in enumerate(zip(uvs_all, mat_ids_all)):\n",
    "            hsv_texture = hsv_textures[material_id]\n",
    "            tex_h, tex_w = hsv_texture.shape[:2]\n",
    "            pixel_coords = (uv_coords * [tex_w, tex_h]).astype(int)\n",
    "            pixel_coords = np.clip(pixel_coords, 0, [tex_w - 1, tex_h - 1])\n",
    "            min_x = np.min(pixel_coords[:, 0])\n",
    "            max_x = np.max(pixel_coords[:, 0])\n",
    "            min_y = np.min(pixel_coords[:, 1])\n",
    "            max_y = np.max(pixel_coords[:, 1])\n",
    "            cropped_hsv = hsv_texture[min_y:max_y+1, min_x:max_x+1]\n",
    "            # 构造局部掩膜：仅保留三角形内的区域\n",
    "            local_mask = np.zeros((max_y - min_y + 1, max_x - min_x + 1), dtype=np.uint8)\n",
    "            local_pixel_coords = pixel_coords.copy()\n",
    "            local_pixel_coords[:, 0] -= min_x\n",
    "            local_pixel_coords[:, 1] -= min_y\n",
    "            cv2.fillPoly(local_mask, [local_pixel_coords], 255)\n",
    "            cropped_rgb = cv2.cvtColor(cropped_hsv, cv2.COLOR_HSV2RGB)\n",
    "            black_background = np.zeros_like(cropped_rgb)\n",
    "            black_background[local_mask == 255] = cropped_rgb[local_mask == 255]\n",
    "            img_pil = Image.fromarray(black_background)\n",
    "            triangle_texture_images.append(img_pil)\n",
    "\n",
    "        # =============================================================================\n",
    "        # 4. 将每个三角形纹理粘贴回对应材质的全黑纹理图中\n",
    "        # =============================================================================\n",
    "        material_ids = np.asarray(mesh_veg.triangle_material_ids)\n",
    "        unique_mat_ids = np.unique(material_ids)\n",
    "        combined_textures = {}\n",
    "        for mat_id in unique_mat_ids:\n",
    "            orig_texture_np = np.asarray(mesh_textures[mat_id])\n",
    "            h, w = orig_texture_np.shape[:2]\n",
    "            combined_textures[mat_id] = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "\n",
    "        uvs_all = np.asarray(mesh_veg.triangle_uvs).reshape(-1, 3, 2)\n",
    "        mat_ids_all = np.asarray(mesh_veg.triangle_material_ids)\n",
    "        for i, (uv_coords, mat_id) in enumerate(zip(uvs_all, mat_ids_all)):\n",
    "            orig_texture_np = np.asarray(mesh_textures[mat_id])\n",
    "            tex_h, tex_w = orig_texture_np.shape[:2]\n",
    "            pixel_coords = (uv_coords * [tex_w, tex_h]).astype(int)\n",
    "            pixel_coords = np.clip(pixel_coords, 0, [tex_w - 1, tex_h - 1])\n",
    "            min_x = np.min(pixel_coords[:, 0])\n",
    "            max_x = np.max(pixel_coords[:, 0])\n",
    "            min_y = np.min(pixel_coords[:, 1])\n",
    "            max_y = np.max(pixel_coords[:, 1])\n",
    "            tri_img = np.array(triangle_texture_images[i])\n",
    "            # 生成非黑区域掩膜\n",
    "            mask = (tri_img.sum(axis=2) > 0)\n",
    "            target_region = combined_textures[mat_id][min_y:max_y+1, min_x:max_x+1]\n",
    "            if target_region.shape[:2] != tri_img.shape[:2]:\n",
    "                print(f\"Warning: Region shape {target_region.shape[:2]} differs from triangle image shape {tri_img.shape[:2]} at triangle {i}.\")\n",
    "                continue\n",
    "            target_region[mask] = tri_img[mask]\n",
    "            combined_textures[mat_id][min_y:max_y+1, min_x:max_x+1] = target_region\n",
    "\n",
    "        # =============================================================================\n",
    "        # 5. 对拼接后的纹理图提取图斑，并对每个图斑进行色调扰动\n",
    "        # =============================================================================\n",
    "        print('纹理扰动中')\n",
    "\n",
    "        # 这里以第一个材质的拼接图为例\n",
    "        combined_img = np.array(list(combined_textures.values())[0])\n",
    "        patch_infos = extract_patches_with_bbox(combined_img)\n",
    "        \n",
    "\n",
    "        asymmetric_jitter = AsymmetricHueJitter(hue_range=(-0.3, 0.15))\n",
    "        jittered_patch_infos = []\n",
    "        for (patch, x, y, w, h) in patch_infos:\n",
    "            patch_pil = Image.fromarray(patch)\n",
    "            jittered_img = asymmetric_jitter(patch_pil)\n",
    "            jittered_patch = np.array(jittered_img)\n",
    "            jittered_patch_infos.append((jittered_patch, x, y, w, h))\n",
    "\n",
    "        # =============================================================================\n",
    "        # 6. 重新拼接扰动后的图斑到大图中，注意只替换非黑区域\n",
    "        # =============================================================================\n",
    "        reassembled_img = np.asarray(read_mesh.textures[1])\n",
    "        for (jittered_patch, x, y, w, h) in jittered_patch_infos:\n",
    "            # 生成非黑区域掩膜\n",
    "            mask = (jittered_patch.sum(axis=2) > 0)\n",
    "            region = reassembled_img[y:y+h, x:x+w]\n",
    "            region[mask] = jittered_patch[mask]\n",
    "            reassembled_img[y:y+h, x:x+w] = region\n",
    "\n",
    "        new_mesh = o3d.geometry.TriangleMesh()\n",
    "        new_mesh.vertices = read_mesh.vertices\n",
    "        new_mesh.triangles = read_mesh.triangles\n",
    "        new_mesh.triangle_material_ids = read_mesh.triangle_material_ids\n",
    "        new_mesh.triangle_uvs = read_mesh.triangle_uvs\n",
    "        # =============================================================================\n",
    "        # 7. 将扰动后的纹理更新到网格中并可视化\n",
    "        # =============================================================================\n",
    "        # 确保图像为uint8类型\n",
    "        if reassembled_img.dtype != np.uint8:\n",
    "            reassembled_img = (reassembled_img * 255).astype(np.uint8)\n",
    "        texture_o3d = o3d.geometry.Image(reassembled_img)\n",
    "        new_mesh.textures = [texture_o3d, texture_o3d]\n",
    "        new_mesh.compute_vertex_normals()\n",
    "        # print(\"使用扰动后的纹理可视化网格模型...\")\n",
    "    print('数据导出中')\n",
    "    export_vertices = np.asarray(new_mesh.vertices)\n",
    "    export_faces = np.asarray(new_mesh.triangles)\n",
    "    export_mesh = trimesh.Trimesh(vertices=export_vertices, faces=export_faces, process=False)\n",
    "\n",
    "    uvs_per_triangle = np.asarray(new_mesh.triangle_uvs)  # 每个三角形的 3 个 UV\n",
    "    uvs_per_vertex = np.zeros((export_vertices.shape[0], 2))  # 创建 per-vertex UV 容器\n",
    "\n",
    "    # 遍历 faces，把每个 face 的 UV 映射到对应的 vertex\n",
    "    tri_vertex_indices = export_faces.flatten()  # 获取所有顶点索引 (3 * n_faces)\n",
    "    uvs_per_vertex[tri_vertex_indices] = uvs_per_triangle  # 直接映射 UV\n",
    "    export_uvs = uvs_per_vertex\n",
    "    texture_np = np.asarray(new_mesh.textures[0])  # Open3D 纹理转换为 numpy\n",
    "    im = Image.fromarray(texture_np)\n",
    "    export_img = ImageOps.flip(im)\n",
    "    texture_visuals = trimesh.visual.texture.TextureVisuals(uv=export_uvs, image=export_img)\n",
    "    export_mesh.visual = texture_visuals  # 绑定到 trimesh 对象\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    \n",
    "    # 如果网格有材质信息，修改材质名称和纹理文件名\n",
    "    if hasattr(export_mesh.visual, 'material') and export_mesh.visual.material is not None:\n",
    "        export_mesh.visual.material.name = output_texturepath\n",
    "        # export_mesh.visual.material.image_name = os.path.basename(output_texturepath)\n",
    "\n",
    "    # 导出 OBJ 文件（trimesh 会自动生成对应的 MTL 文件，并写入材质信息）\n",
    "    try:\n",
    "        print(\"正在导出文件...\")\n",
    "        export_mesh.export(output_objpath, file_type='obj', mtl_name=identifier + \"_jittered.mtl\")\n",
    "    except Exception as e:\n",
    "        print(f\"导出文件出错: {output_objpath}\\n错误信息: {e}\")\n",
    "    if new_mesh is None:\n",
    "        print(\"未生成有效的 new_mesh，跳过导出。\")\n",
    "        return\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    obj_filepath = r'D:\\data_cfd\\SUM_Helsinki_C6_mesh\\SUM_Helsinki_C6_mesh\\test_obj'\n",
    "    txt_file_path = r'D:\\data_cfd\\SUM_Helsinki_C6_mesh\\SUM_Helsinki_C6_mesh\\test_obj'\n",
    "    output_path = r'D:\\data_cfd\\SUM_Helsinki_C6_mesh\\SUM_Helsinki_C6_mesh\\test_jitter'\n",
    "    jitter_label = 2\n",
    "\n",
    "    abs_output = os.path.abspath(output_path)\n",
    "# o3d.io.\n",
    "    # 第一步：遍历目录树，收集所有待处理的 OBJ 文件\n",
    "    obj_files = []\n",
    "    for root, dirs, files in os.walk(obj_filepath):\n",
    "        # 如果当前目录在输出目录中，则跳过\n",
    "        if os.path.abspath(root).startswith(abs_output):\n",
    "            print(\"输出与搜索路径发生嵌套，将不对生成的文件进行处理。\")\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith('.obj') and \"_jittered\" not in file:\n",
    "                obj_files.append(os.path.join(root, file))\n",
    "\n",
    "    # 第二步：使用全局进度条处理所有收集到的 OBJ 文件\n",
    "    for obj_file in tqdm(obj_files, desc=\"Processing each OBJ file\", total=len(obj_files)):\n",
    "        print('正在处理', obj_file)\n",
    "        label_jitter(obj_file, txt_file_path, output_path, jitter_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
